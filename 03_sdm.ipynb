{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/marinebon/HackingLimno2025/blob/main/03_sdm.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "install dependencies TODO: are all these needed?\n"
      ],
      "metadata": {
        "id": "RlxwTxhoDvbb"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "duORdW0vAiYc"
      },
      "outputs": [],
      "source": [
        "!pip install  rasterio pandas geopandas matplotlib  requests xarray netCDF4"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Import libraries & setup\n"
      ],
      "metadata": {
        "id": "PA9meCsJEJJD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO: are all these imports needed?\n",
        "\n",
        "\n",
        "# Standard library imports\n",
        "from datetime import datetime\n",
        "from typing import Dict, List, Optional, Tuple, Union\n",
        "import warnings\n",
        "\n",
        "# Third-party library imports\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import rasterio\n",
        "import seaborn as sns\n",
        "import xarray as xr\n",
        "from scipy.spatial.distance import cdist\n",
        "from sklearn import metrics, svm\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "%matplotlib inline\n",
        "\n",
        "# google drive connection\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "import os\n",
        "project_dir = '/content/drive/MyDrive/GSoC_SDM_Project'\n",
        "if not os.path.exists(project_dir):\n",
        "    os.makedirs(project_dir)"
      ],
      "metadata": {
        "id": "eroPo3d2EEx4"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "SDM Class for Marine species\n"
      ],
      "metadata": {
        "id": "_Kl5xGZ72gLH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MarineSpeciesDistributionModel:\n",
        "    \"\"\"\n",
        "    A Species Distribution Model for marine species using One-Class SVM\n",
        "    Similar to the scikit-learn species distribution example but adapted for marine data\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, nu=0.1, kernel='rbf', gamma=0.5):\n",
        "        \"\"\"\n",
        "        Initialize the SDM model\n",
        "\n",
        "        Parameters:\n",
        "        -----------\n",
        "        nu : float, default=0.1\n",
        "            An upper bound on the fraction of training errors and a lower bound of the fraction of support vectors\n",
        "        kernel : str, default='rbf'\n",
        "            Kernel type for SVM\n",
        "        gamma : float, default=0.5\n",
        "            Kernel coefficient for 'rbf'\n",
        "        \"\"\"\n",
        "        self.nu = nu\n",
        "        self.kernel = kernel\n",
        "        self.gamma = gamma\n",
        "        self.model = None\n",
        "        self.scaler = None\n",
        "        self.feature_names = None\n",
        "        self.species_name = None\n",
        "\n",
        "    def add_environmental_data(self, df, raster_path, column_name='environmental_var',\n",
        "                              lat_col='decimalLatitude', lon_col='decimalLongitude'):\n",
        "        \"\"\"\n",
        "        Add environmental data from raster to occurrence dataframe\n",
        "        \"\"\"\n",
        "        df_result = df.copy()\n",
        "\n",
        "        try:\n",
        "            with rasterio.open(raster_path) as src:\n",
        "                coords = [(row[lon_col], row[lat_col]) for _, row in df.iterrows()]\n",
        "                sampled_values = list(src.sample(coords))\n",
        "                values = [val[0] if val[0] != src.nodata else np.nan for val in sampled_values]\n",
        "                df_result[column_name] = values\n",
        "\n",
        "            valid_count = df_result[column_name].notna().sum()\n",
        "            print(f\"Added {column_name}: {valid_count}/{len(df_result)} valid values\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error adding environmental data from {raster_path}: {e}\")\n",
        "            df_result[column_name] = np.nan\n",
        "\n",
        "        return df_result\n",
        "\n",
        "    def prepare_data(self, df, environmental_cols, lat_col='decimalLatitude',\n",
        "                    lon_col='decimalLongitude', test_size=0.2, random_state=42):\n",
        "        \"\"\"\n",
        "        Prepare training and testing data from occurrence dataframe\n",
        "        \"\"\"\n",
        "        df_clean = df.dropna(subset=environmental_cols)\n",
        "\n",
        "        if len(df_clean) == 0:\n",
        "            raise ValueError(\"No valid data points after removing missing values\")\n",
        "\n",
        "\n",
        "        coords = df_clean[[lat_col, lon_col]].values\n",
        "        env_features = df_clean[environmental_cols].values\n",
        "\n",
        "\n",
        "        X_train, X_test, coords_train, coords_test = train_test_split(\n",
        "            env_features, coords, test_size=test_size, random_state=random_state\n",
        "        )\n",
        "\n",
        "        self.feature_names = environmental_cols\n",
        "\n",
        "        return {\n",
        "            'X_train': X_train,\n",
        "            'X_test': X_test,\n",
        "            'coords_train': coords_train,\n",
        "            'coords_test': coords_test,\n",
        "            'df_clean': df_clean\n",
        "        }\n",
        "\n",
        "    def fit(self, X_train):\n",
        "        \"\"\"\n",
        "        Fit the One-Class SVM model\n",
        "        \"\"\"\n",
        "\n",
        "        self.scaler = StandardScaler()\n",
        "        X_train_scaled = self.scaler.fit_transform(X_train)\n",
        "\n",
        "\n",
        "        print(\"Fitting One-Class SVM...\")\n",
        "        self.model = svm.OneClassSVM(nu=self.nu, kernel=self.kernel, gamma=self.gamma)\n",
        "        self.model.fit(X_train_scaled)\n",
        "        print(\"Model fitting completed.\")\n",
        "\n",
        "    def predict(self, X):\n",
        "        \"\"\"\n",
        "        Predict species suitability for given environmental conditions\n",
        "        \"\"\"\n",
        "        if self.model is None or self.scaler is None:\n",
        "            raise ValueError(\"Model must be fitted before prediction\")\n",
        "\n",
        "        X_scaled = self.scaler.transform(X)\n",
        "        return self.model.decision_function(X_scaled)\n",
        "\n",
        "    def create_prediction_grid(self, raster_paths, grid_resolution=0.1,\n",
        "                              bounds=None, column_names=None):\n",
        "        \"\"\"\n",
        "        Create a prediction grid from environmental rasters\n",
        "\n",
        "        Parameters:\n",
        "        -----------\n",
        "        raster_paths : list\n",
        "            List of paths to environmental raster files\n",
        "        grid_resolution : float\n",
        "            Resolution of the prediction grid in degrees\n",
        "        bounds : tuple\n",
        "            (min_lon, min_lat, max_lon, max_lat) bounds for the grid\n",
        "        column_names : list\n",
        "            Names for the environmental variables\n",
        "        \"\"\"\n",
        "        if column_names is None:\n",
        "            column_names = [f'env_var_{i}' for i in range(len(raster_paths))]\n",
        "\n",
        "\n",
        "        if bounds is None:\n",
        "            with rasterio.open(raster_paths[0]) as src:\n",
        "                bounds = src.bounds\n",
        "                bounds = (bounds.left, bounds.bottom, bounds.right, bounds.top)\n",
        "\n",
        "        min_lon, min_lat, max_lon, max_lat = bounds\n",
        "\n",
        "\n",
        "        lons = np.arange(min_lon, max_lon, grid_resolution)\n",
        "        lats = np.arange(min_lat, max_lat, grid_resolution)\n",
        "        lon_grid, lat_grid = np.meshgrid(lons, lats)\n",
        "\n",
        "\n",
        "        coords_flat = np.column_stack([lon_grid.ravel(), lat_grid.ravel()])\n",
        "\n",
        "\n",
        "        env_data = []\n",
        "        for i, raster_path in enumerate(raster_paths):\n",
        "            try:\n",
        "                with rasterio.open(raster_path) as src:\n",
        "                    sampled_values = list(src.sample(coords_flat))\n",
        "                    values = [val[0] if val[0] != src.nodata else np.nan for val in sampled_values]\n",
        "                    env_data.append(values)\n",
        "                    print(f\"Sampled {column_names[i]}: {sum(~np.isnan(values))}/{len(values)} valid values\")\n",
        "            except Exception as e:\n",
        "                print(f\"Error sampling from {raster_path}: {e}\")\n",
        "                env_data.append([np.nan] * len(coords_flat))\n",
        "\n",
        "\n",
        "        grid_df = pd.DataFrame({\n",
        "            'longitude': coords_flat[:, 0],\n",
        "            'latitude': coords_flat[:, 1]\n",
        "        })\n",
        "\n",
        "        for i, col_name in enumerate(column_names):\n",
        "            grid_df[col_name] = env_data[i]\n",
        "\n",
        "\n",
        "        grid_df_clean = grid_df.dropna()\n",
        "\n",
        "        return grid_df_clean, (lons, lats)\n",
        "\n",
        "    def evaluate_model(self, X_test, background_points_env=None, n_background=1000):\n",
        "        \"\"\"\n",
        "        Evaluate model performance using AUC\n",
        "        \"\"\"\n",
        "\n",
        "        pred_test = self.predict(X_test)\n",
        "\n",
        "        if background_points_env is None:\n",
        "            feature_mins = np.min(np.vstack([X_test, self.scaler.transform(X_test)]), axis=0)\n",
        "            feature_maxs = np.max(np.vstack([X_test, self.scaler.transform(X_test)]), axis=0)\n",
        "\n",
        "            background_points_env = np.random.uniform(\n",
        "                low=feature_mins, high=feature_maxs,\n",
        "                size=(n_background, X_test.shape[1])\n",
        "            )\n",
        "\n",
        "        pred_background = self.predict(background_points_env)\n",
        "\n",
        "        scores = np.concatenate([pred_test, pred_background])\n",
        "        y_true = np.concatenate([np.ones(len(pred_test)), np.zeros(len(pred_background))])\n",
        "\n",
        "\n",
        "        fpr, tpr, thresholds = metrics.roc_curve(y_true, scores)\n",
        "        auc_score = metrics.auc(fpr, tpr)\n",
        "\n",
        "        return auc_score, fpr, tpr\n",
        "\n",
        "    def plot_distribution_map(self, grid_df, prediction_col='suitability',\n",
        "                             occurrence_data=None, figsize=(12, 8)):\n",
        "        \"\"\"\n",
        "        Plot the species distribution map\n",
        "        \"\"\"\n",
        "        plt.figure(figsize=figsize)\n",
        "\n",
        "\n",
        "        scatter = plt.scatter(grid_df['longitude'], grid_df['latitude'],\n",
        "                            c=grid_df[prediction_col], cmap='Reds',\n",
        "                            s=1, alpha=0.6)\n",
        "        plt.colorbar(scatter, label='Habitat Suitability')\n",
        "\n",
        "        if occurrence_data is not None:\n",
        "            if 'coords_train' in occurrence_data:\n",
        "                plt.scatter(occurrence_data['coords_train'][:, 1],\n",
        "                          occurrence_data['coords_train'][:, 0],\n",
        "                          c='black', marker='^', s=20, label='Training', alpha=0.8)\n",
        "\n",
        "            if 'coords_test' in occurrence_data:\n",
        "                plt.scatter(occurrence_data['coords_test'][:, 1],\n",
        "                          occurrence_data['coords_test'][:, 0],\n",
        "                          c='blue', marker='x', s=20, label='Testing', alpha=0.8)\n",
        "\n",
        "            plt.legend()\n",
        "\n",
        "        plt.xlabel('Longitude')\n",
        "        plt.ylabel('Latitude')\n",
        "        plt.title(f'Species Distribution Model: {self.species_name or \"Unknown Species\"}')\n",
        "        plt.grid(True, alpha=0.3)\n",
        "\n",
        "        return plt.gcf()\n"
      ],
      "metadata": {
        "id": "lV-3h7Nt2yBy"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_final = # TODO: open csv file from gdrive\n",
        "environmental_cols = ['salinity']\n",
        "\n",
        "if add_more_variables:\n",
        "\n",
        "\n",
        "    potential_files = [\n",
        "        ('temperature.nc', 'temperature'),\n",
        "        ('sst.nc', 'sea_surface_temperature'),\n",
        "        ('depth.nc', 'depth'),\n",
        "        ('bathymetry.nc', 'bathymetry'),\n",
        "        ('chlorophyll.nc', 'chlorophyll'),\n",
        "        ('oxygen.nc', 'oxygen'),\n",
        "        ('current_speed.nc', 'current_speed')\n",
        "    ]\n",
        "\n",
        "    for filename, var_name in potential_files:\n",
        "        try:\n",
        "            raster_path = f'{project_dir}/{filename}'\n",
        "            df_final = add_environmental_data(df_final, raster_path, var_name)\n",
        "            environmental_cols.append(var_name)\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\" Could not add {var_name}: file not found or error\")\n",
        "            continue\n",
        "\n",
        "print(\"\\n Preparing data for modeling...\")\n",
        "\n",
        "df_clean = df_final.dropna(subset=environmental_cols)\n",
        "print(f\" Clean records: {len(df_clean)}/{len(df_final)} ({len(df_clean)/len(df_final)*100:.1f}%)\")\n",
        "\n",
        "# if len(df_clean) < 20:\n",
        "#     print(\" Error: Too few records with complete environmental data!\")\n",
        "#     return None\n",
        "\n",
        "\n",
        "X = df_clean[environmental_cols].values\n",
        "coords = df_clean[['decimalLatitude', 'decimalLongitude']].values\n",
        "\n",
        "# Split into train/test\n",
        "X_train, X_test, coords_train, coords_test = train_test_split(\n",
        "    X, coords, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "print(f\" Training samples: {len(X_train)}\")\n",
        "print(f\" Testing samples: {len(X_test)}\")\n",
        "\n",
        "\n",
        "print(\"\\n Environmental Data Summary:\")\n",
        "for i, col in enumerate(environmental_cols):\n",
        "    data = df_clean[col]\n",
        "    print(f\"  {col}: {data.min():.2f} - {data.max():.2f} (mean: {data.mean():.2f})\")\n",
        "\n",
        "print(\"\\n Training One-Class SVM model...\")\n",
        "\n",
        "\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "\n",
        "\n",
        "model = svm.OneClassSVM(nu=0.1, kernel='rbf', gamma='scale')\n",
        "model.fit(X_train_scaled)\n",
        "\n",
        "print(\"Model training completed!\")\n",
        "\n",
        "print(\"\\n Evaluating model performance...\")\n",
        "\n",
        "\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "test_predictions = model.decision_function(X_test_scaled)\n",
        "\n",
        "n_background = 1000\n",
        "feature_mins = X_train.min(axis=0)\n",
        "feature_maxs = X_train.max(axis=0)\n",
        "\n",
        "background_points = np.random.uniform(\n",
        "    low=feature_mins,\n",
        "    high=feature_maxs,\n",
        "    size=(n_background, len(environmental_cols))\n",
        ")\n",
        "\n",
        "background_scaled = scaler.transform(background_points)\n",
        "background_predictions = model.decision_function(background_scaled)\n",
        "\n",
        "\n",
        "y_true = np.concatenate([np.ones(len(test_predictions)), np.zeros(len(background_predictions))])\n",
        "y_scores = np.concatenate([test_predictions, background_predictions])\n",
        "\n",
        "fpr, tpr, _ = metrics.roc_curve(y_true, y_scores)\n",
        "auc_score = metrics.auc(fpr, tpr)\n",
        "\n",
        "print(f\" AUC Score: {auc_score:.3f}\")\n",
        "\n",
        "\n",
        "print(\"\\n Creating habitat suitability predictions...\")\n",
        "\n",
        "X_all_scaled = scaler.transform(X)\n",
        "all_predictions = model.decision_function(X_all_scaled)\n",
        "\n",
        "df_results = df_clean.copy()\n",
        "df_results['habitat_suitability'] = all_predictions\n",
        "\n",
        "print(f\" Suitability range: {all_predictions.min():.3f} to {all_predictions.max():.3f}\")\n",
        "\n",
        "\n",
        "print(\"\\n Creating visualizations...\")\n",
        "\n",
        "plt.figure(figsize=(10, 4))\n",
        "\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (AUC = {auc_score:.3f})')\n",
        "plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
        "plt.xlim([0.0, 1.0])\n",
        "plt.ylim([0.0, 1.05])\n",
        "plt.xlabel('False Positive Rate')\n",
        "plt.ylabel('True Positive Rate')\n",
        "plt.title('ROC Curve - Mola mola SDM')\n",
        "plt.legend(loc=\"lower right\")\n",
        "plt.grid(True, alpha=0.3)\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "scatter = plt.scatter(\n",
        "    df_results['decimalLongitude'],\n",
        "    df_results['decimalLatitude'],\n",
        "    c=df_results['habitat_suitability'],\n",
        "    cmap='RdYlBu_r',\n",
        "    s=30,\n",
        "    alpha=0.7,\n",
        "    edgecolors='black',\n",
        "    linewidth=0.5\n",
        ")\n",
        "plt.colorbar(scatter, label='Habitat Suitability')\n",
        "plt.xlabel('Longitude')\n",
        "plt.ylabel('Latitude')\n",
        "plt.title('Mola mola - Habitat Suitability')\n",
        "plt.grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "\n",
        "print(\"\\n ANALYSIS SUMMARY\")\n",
        "\n",
        "print(f\" Species: Mola mola\")\n",
        "print(f\" Total records: {len(df_with_env)}\")\n",
        "print(f\" Records used: {len(df_clean)}\")\n",
        "print(f\" Environmental variables: {len(environmental_cols)}\")\n",
        "print(f\" Model performance (AUC): {auc_score:.3f}\")\n",
        "\n",
        "if auc_score > 0.7:\n",
        "    print(\" Good model performance!\")\n",
        "elif auc_score > 0.6:\n",
        "    print(\"  Moderate model performance\")\n",
        "else:\n",
        "    print(\" Poor model performance - consider adding more environmental variables\")\n",
        "\n",
        "\n",
        "print(f\"\\n TOP 5 MOST SUITABLE LOCATIONS:\")\n",
        "top_locations = df_results.nlargest(5, 'habitat_suitability')\n",
        "for i, (idx, row) in enumerate(top_locations.iterrows(), 1):\n",
        "    print(f\"  {i}. Lat: {row['decimalLatitude']:.3f}, Lon: {row['decimalLongitude']:.3f}, \"\n",
        "            f\"Suitability: {row['habitat_suitability']:.3f}\")\n",
        "\n",
        "print(f\"\\n BOTTOM 5 LEAST SUITABLE LOCATIONS:\")\n",
        "bottom_locations = df_results.nsmallest(5, 'habitat_suitability')\n",
        "for i, (idx, row) in enumerate(bottom_locations.iterrows(), 1):\n",
        "    print(f\"  {i}. Lat: {row['decimalLatitude']:.3f}, Lon: {row['decimalLongitude']:.3f}, \"\n",
        "            f\"Suitability: {row['habitat_suitability']:.3f}\")\n",
        "\n"
      ],
      "metadata": {
        "id": "RhDbu1Ht6SNj",
        "outputId": "fe15e352-f337-45d5-b3f6-9f04ceefd418",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 219
        }
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'df_with_env' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-18-1517127240.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf_final\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf_with_env\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0menvironmental_cols\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'salinity'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0madd_more_variables\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'df_with_env' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "An Example how to use the sdm class\n"
      ],
      "metadata": {
        "id": "0-GqZ24m3qfh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "SDM for mola mola"
      ],
      "metadata": {
        "id": "j-sQ0nYZ4HRc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Example usage\n"
      ],
      "metadata": {
        "id": "Yqzg0lwY8RwP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def run_mola_mola_sdm(df_with_env, project_dir='.', add_more_variables=True):\n",
        "\n",
        "    df_final = df_with_env.copy()\n",
        "    environmental_cols = ['salinity']\n",
        "\n",
        "    if add_more_variables:\n",
        "\n",
        "\n",
        "        potential_files = [\n",
        "            ('temperature.nc', 'temperature'),\n",
        "            ('sst.nc', 'sea_surface_temperature'),\n",
        "            ('depth.nc', 'depth'),\n",
        "            ('bathymetry.nc', 'bathymetry'),\n",
        "            ('chlorophyll.nc', 'chlorophyll'),\n",
        "            ('oxygen.nc', 'oxygen'),\n",
        "            ('current_speed.nc', 'current_speed')\n",
        "        ]\n",
        "\n",
        "        for filename, var_name in potential_files:\n",
        "            try:\n",
        "                raster_path = f'{project_dir}/{filename}'\n",
        "                df_final = add_environmental_data(df_final, raster_path, var_name)\n",
        "                environmental_cols.append(var_name)\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\" Could not add {var_name}: file not found or error\")\n",
        "                continue\n",
        "\n",
        "    print(\"\\n Preparing data for modeling...\")\n",
        "\n",
        "    df_clean = df_final.dropna(subset=environmental_cols)\n",
        "    print(f\" Clean records: {len(df_clean)}/{len(df_final)} ({len(df_clean)/len(df_final)*100:.1f}%)\")\n",
        "\n",
        "    # if len(df_clean) < 20:\n",
        "    #     print(\" Error: Too few records with complete environmental data!\")\n",
        "    #     return None\n",
        "\n",
        "\n",
        "    X = df_clean[environmental_cols].values\n",
        "    coords = df_clean[['decimalLatitude', 'decimalLongitude']].values\n",
        "\n",
        "    # Split into train/test\n",
        "    X_train, X_test, coords_train, coords_test = train_test_split(\n",
        "        X, coords, test_size=0.2, random_state=42\n",
        "    )\n",
        "\n",
        "    print(f\" Training samples: {len(X_train)}\")\n",
        "    print(f\" Testing samples: {len(X_test)}\")\n",
        "\n",
        "\n",
        "    print(\"\\n Environmental Data Summary:\")\n",
        "    for i, col in enumerate(environmental_cols):\n",
        "        data = df_clean[col]\n",
        "        print(f\"  {col}: {data.min():.2f} - {data.max():.2f} (mean: {data.mean():.2f})\")\n",
        "\n",
        "    print(\"\\n Training One-Class SVM model...\")\n",
        "\n",
        "\n",
        "    scaler = StandardScaler()\n",
        "    X_train_scaled = scaler.fit_transform(X_train)\n",
        "\n",
        "\n",
        "    model = svm.OneClassSVM(nu=0.1, kernel='rbf', gamma='scale')\n",
        "    model.fit(X_train_scaled)\n",
        "\n",
        "    print(\"Model training completed!\")\n",
        "\n",
        "    print(\"\\n Evaluating model performance...\")\n",
        "\n",
        "\n",
        "    X_test_scaled = scaler.transform(X_test)\n",
        "    test_predictions = model.decision_function(X_test_scaled)\n",
        "\n",
        "    n_background = 1000\n",
        "    feature_mins = X_train.min(axis=0)\n",
        "    feature_maxs = X_train.max(axis=0)\n",
        "\n",
        "    background_points = np.random.uniform(\n",
        "        low=feature_mins,\n",
        "        high=feature_maxs,\n",
        "        size=(n_background, len(environmental_cols))\n",
        "    )\n",
        "\n",
        "    background_scaled = scaler.transform(background_points)\n",
        "    background_predictions = model.decision_function(background_scaled)\n",
        "\n",
        "\n",
        "    y_true = np.concatenate([np.ones(len(test_predictions)), np.zeros(len(background_predictions))])\n",
        "    y_scores = np.concatenate([test_predictions, background_predictions])\n",
        "\n",
        "    fpr, tpr, _ = metrics.roc_curve(y_true, y_scores)\n",
        "    auc_score = metrics.auc(fpr, tpr)\n",
        "\n",
        "    print(f\" AUC Score: {auc_score:.3f}\")\n",
        "\n",
        "\n",
        "    print(\"\\n Creating habitat suitability predictions...\")\n",
        "\n",
        "    X_all_scaled = scaler.transform(X)\n",
        "    all_predictions = model.decision_function(X_all_scaled)\n",
        "\n",
        "    df_results = df_clean.copy()\n",
        "    df_results['habitat_suitability'] = all_predictions\n",
        "\n",
        "    print(f\" Suitability range: {all_predictions.min():.3f} to {all_predictions.max():.3f}\")\n",
        "\n",
        "\n",
        "    print(\"\\n Creating visualizations...\")\n",
        "\n",
        "    plt.figure(figsize=(10, 4))\n",
        "\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (AUC = {auc_score:.3f})')\n",
        "    plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
        "    plt.xlim([0.0, 1.0])\n",
        "    plt.ylim([0.0, 1.05])\n",
        "    plt.xlabel('False Positive Rate')\n",
        "    plt.ylabel('True Positive Rate')\n",
        "    plt.title('ROC Curve - Mola mola SDM')\n",
        "    plt.legend(loc=\"lower right\")\n",
        "    plt.grid(True, alpha=0.3)\n",
        "\n",
        "    plt.subplot(1, 2, 2)\n",
        "    scatter = plt.scatter(\n",
        "        df_results['decimalLongitude'],\n",
        "        df_results['decimalLatitude'],\n",
        "        c=df_results['habitat_suitability'],\n",
        "        cmap='RdYlBu_r',\n",
        "        s=30,\n",
        "        alpha=0.7,\n",
        "        edgecolors='black',\n",
        "        linewidth=0.5\n",
        "    )\n",
        "    plt.colorbar(scatter, label='Habitat Suitability')\n",
        "    plt.xlabel('Longitude')\n",
        "    plt.ylabel('Latitude')\n",
        "    plt.title('Mola mola - Habitat Suitability')\n",
        "    plt.grid(True, alpha=0.3)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "    print(\"\\n ANALYSIS SUMMARY\")\n",
        "\n",
        "    print(f\" Species: Mola mola\")\n",
        "    print(f\" Total records: {len(df_with_env)}\")\n",
        "    print(f\" Records used: {len(df_clean)}\")\n",
        "    print(f\" Environmental variables: {len(environmental_cols)}\")\n",
        "    print(f\" Model performance (AUC): {auc_score:.3f}\")\n",
        "\n",
        "    if auc_score > 0.7:\n",
        "        print(\" Good model performance!\")\n",
        "    elif auc_score > 0.6:\n",
        "        print(\"  Moderate model performance\")\n",
        "    else:\n",
        "        print(\" Poor model performance - consider adding more environmental variables\")\n",
        "\n",
        "\n",
        "    print(f\"\\n TOP 5 MOST SUITABLE LOCATIONS:\")\n",
        "    top_locations = df_results.nlargest(5, 'habitat_suitability')\n",
        "    for i, (idx, row) in enumerate(top_locations.iterrows(), 1):\n",
        "        print(f\"  {i}. Lat: {row['decimalLatitude']:.3f}, Lon: {row['decimalLongitude']:.3f}, \"\n",
        "              f\"Suitability: {row['habitat_suitability']:.3f}\")\n",
        "\n",
        "    print(f\"\\n BOTTOM 5 LEAST SUITABLE LOCATIONS:\")\n",
        "    bottom_locations = df_results.nsmallest(5, 'habitat_suitability')\n",
        "    for i, (idx, row) in enumerate(bottom_locations.iterrows(), 1):\n",
        "        print(f\"  {i}. Lat: {row['decimalLatitude']:.3f}, Lon: {row['decimalLongitude']:.3f}, \"\n",
        "              f\"Suitability: {row['habitat_suitability']:.3f}\")\n",
        "\n",
        "\n",
        "    results = {\n",
        "        'model': model,\n",
        "        'scaler': scaler,\n",
        "        'predictions': df_results,\n",
        "        'auc_score': auc_score,\n",
        "        'environmental_cols': environmental_cols,\n",
        "        'train_data': X_train,\n",
        "        'test_data': X_test,\n",
        "        'train_coords': coords_train,\n",
        "        'test_coords': coords_test,\n",
        "        'fpr': fpr,\n",
        "        'tpr': tpr\n",
        "    }\n",
        "\n",
        "\n",
        "\n",
        "    return results\n"
      ],
      "metadata": {
        "id": "3De7XUK78TqR"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Usage"
      ],
      "metadata": {
        "id": "WQ7SnORpAbkH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def example_basic_usage(df_with_env, project_dir):\n",
        "    \"\"\"\n",
        "    Basic example - just run with salinity data\n",
        "    \"\"\"\n",
        "\n",
        "    results = run_mola_mola_sdm(df_with_env, project_dir, add_more_variables=False)\n",
        "\n",
        "    if results:\n",
        "        print(\"Analysis completed!\")\n",
        "        print(f\" AUC Score: {results['auc_score']:.3f}\")\n",
        "        return results\n",
        "    else:\n",
        "        print(\"Analysis failed!\")\n",
        "        return None\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "   example_basic_usage(df_with_env=df_with_env,project_dir=project_dir)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 182
        },
        "id": "CLQke5VvAWTI",
        "outputId": "438b33a1-096f-408c-98c6-8f183c444666"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'df_with_env' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-15-2513806900.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m    \u001b[0mexample_basic_usage\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_with_env\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdf_with_env\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mproject_dir\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mproject_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'df_with_env' is not defined"
          ]
        }
      ]
    }
  ]
}